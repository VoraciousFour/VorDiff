{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Automatic differentiation is a tool for calculating derivatives using machine accuracy. It has several advantages over traditional methods of derivative calculations such as symbolic and finite differentiation. Automatic differentiation is useful for calculating complex derivatives where errors are more likely with classical methods. For instance , with finite differentiation, h values that are too small will lead to accuracy errors though floating point roundoff error, while h values that are too large will start making vastly inaccurate approximations. \n",
    "\n",
    "Automatic differentiation is useful due to its practicality in real world applications that involve thousands of parameters in a complicated function, which would take a long runtime as well as strong possibility for error in calculating the derivatives individually. \n",
    "\n",
    "Our package allows users to calculate derivatives of complex functions, some with many parameters, allowing machine precision.\n",
    "\n",
    "## Background\n",
    "\n",
    "Essentially automatic differentiation works by  breaking down a complicated function and performing a sequence of elementary arithmetic such as addition, subtraction, multiplication, and division as well as elementary functions like exp, log, sin, etc. These operations are then repeated by the chain rule and the derivatives of these sequences are calculated. There are two ways that automatic differentiation can be implemented - forward mode and reverse mode. \n",
    "\n",
    "\n",
    "### 2.1 The Chain Rule\n",
    "\n",
    "The chain rule makes up a fundamental component of auto differentiation. The basic idea is:   \n",
    "For univariate function, $$ F(x) = f(g(x))$$\n",
    " $$F^{\\prime} = (f(g))^{\\prime} = f^{\\prime}(g(x))g^{\\prime}(x)$$\n",
    "For multivariate function, $$F(x) = f(g(x),h(x))$$\n",
    "$$ \\frac{\\partial F}{\\partial x}=\\frac{\\partial f}{\\partial g} \\frac{\\partial g}{\\partial x}+\\frac{\\partial f}{\\partial h} \\frac{\\partial h}{\\partial x}$$\n",
    "For more generalized cases, if F is a combination of more sub-functions,  $$F(x) = f(g_{1}(x), g_{2}(x), â€¦, g_{m}(x))$$\n",
    "$$\\frac{\\partial F}{\\partial x}=\\sum_{i=1}^{m}\\frac{\\partial F}{\\partial g_{i}} \\frac{\\partial g_{i}}{\\partial x}$$\n",
    "\n",
    "### 2.2 Forward Mode\n",
    "\n",
    "The forward mode automatic differentiation is accomplished by firstly splitting the function process into one-by-one steps, each including only one basic operation. Then from the first node, the value and derivative will be calculated based on the values and derivatives of forward nodes. An example of computational graph and table for forward mode AD is shown as follows:\n",
    "\n",
    "![comp-graph](computationalgraph.png)\n",
    "\n",
    "\\begin{align}\n",
    "  f\\left(x,y\\right) =\\sin\\left(xy\\right)\n",
    "\\end{align}\n",
    "We will be evaluating the function at $f(1, 0)$\n",
    "\n",
    "Evaluation trace:\n",
    "\n",
    "| Trace   | Elementary Function      | Current Value           | Elementary Function Derivative       | $\\nabla_{x}$ Value  | $\\nabla_{y}$ Value  |\n",
    "| :---: | :-----------------: | :-----------: | :----------------------------: | :-----------------:  | :-----------------: |\n",
    "| $x_{1}$ | $x_{1}$                  | $1$        | $\\dot{x}_{1}$                        | $1$ | $0$ |\n",
    "| $x_{2}$ | $x_{2}$                  | $0$        | $\\dot{x}_{2}$                        | $0$ | $1$ |\n",
    "| $x_{3}$ | $x_{1}x_{2}$                  | $0$        | $\\dot{x}_{2}$                        | $0$ | $1$ |\n",
    "\n",
    "### 2.3 Reverse Mode\n",
    "\n",
    "The reverse mode automatic differentiation has a process similar to the forward mode auto differentiation, but has another reverse process. It does not apply the chain rule and only partial derivatives to a node are stored. First, for the forward process, the partial derivatives are stored for each node. For the reverse process, it starts with the differentiation to the last node, and then activations in the forward process are deployed in the differentiation differentiation step by step. \n",
    "\n",
    "\n",
    "### 2.4 Forward Mode v.s. Reverse Mode\n",
    "\n",
    "Two main aspects can be considered when choosing between Forward and Reverse mode auto differentiation.\n",
    "* Memory Storage & Time of Computation\n",
    "\n",
    "The forward mode needs memory storage for values and derivatives for each node, while the reverse mode only needs to store the activations of partial differentiation to each node. The forward mode do the computation at the same time as the variable evaluation, while the reverse mode do the calculation in the backward process.\n",
    "* Input & Output Dimensionality\n",
    "\n",
    "If the input dimension is much larger than output dimension, then reverse mode is more attractive. If the output dimension is much larger than the input dimension, the forward mode is much computational cheaper.\n",
    "\n",
    "\n",
    "## How to use VorDiff\n",
    "\n",
    "Our Automatic Differentiation package is called VorDiff. The two main objects you will interact with are imported below:\n",
    "```py\n",
    "from VorDiff import autodiff\n",
    "from VorDiff import operator\n",
    "```\n",
    "In short, the user will first instantiate a variable (scalar or vector) as an `autodiff` object, and then feed those variables to operators in the `operator` object. The user will be able to retrieve the resulting values and derivatives after doing so.\n",
    "\n",
    "The user may instantiate an `autodiff` object by the following:\n",
    "```py\n",
    "x = autodiff.create_scalar(val = 4)\n",
    "v = autodiff.create_vector(vals = [4,2])\n",
    "```\n",
    "Then, the `operator` class can be used to define functions. Simple operations (addition, subtraction, multiplication, and division) may be used normally. More complex functions (e.g. sqrt, sin, cos) must use the operations defined in the `operator` class.\n",
    "```py\n",
    "fx = 4*operator.sqrt(x) + x\n",
    "fv = 4*operator.sqrt(v) + v\n",
    "```\n",
    "The user may retrieve the values and first derivatives from the objects defined above by using the `val` and `der` attributes of the `autodiff` object.\n",
    "```py\n",
    "print(fx.val, fx.der)\n",
    "print(fv.val, fv.der)\n",
    "```\n",
    "For vectors, `val` and `der` will be tuples that contain the values and partial derivatives, respectively, at each element of the vector.\n",
    "\n",
    "## Software Organization\n",
    "\n",
    "### Directory Structure\n",
    "The package's directory will be structured as follows:\n",
    "```\n",
    "VorDiff/\n",
    "\t__init__.py\n",
    "    nodes/\n",
    "\t    node.py\n",
    "\t    scalar.py\n",
    "\t    vector.py\n",
    "\ttests/\n",
    "\t    test_nodes/\n",
    "\t\t    test_node.py\n",
    "\t\t    test_scaler.py\n",
    "\t\t    test_vector.py\n",
    "\t\ttest_autodiff.py\n",
    "        test_operator.py\n",
    "\tdocs/\n",
    "\t\t...\n",
    "\tdemo/\n",
    "\t    ...\n",
    "\tautodiff.py\n",
    "    operator.py\n",
    "    README.md\n",
    "    ...\n",
    "```\n",
    "### Modules\n",
    "-   VorDiff: The VorDiff module contains the operator class to be directly used by users to evaluate functions and calculate their derivatives, a class node and subclasses scalar and vector to be used in autodiff class, and an autodiff class to perform automatic differentiation. This is the core of the package.\n",
    "    \n",
    "-   Test_Vordiff: The Test_Vordiff module contains the test suite for this project. TravisCI and CodeCov are used to test our operator classes, node classes, and auto-differentiator.\n",
    "    \n",
    "-   Demo: The Demo module contains python files demonstrating how to perform automatic differentiation with the implemented functions.\n",
    "    \n",
    "### Testing\n",
    "In this project we will use TravisCI to perform continuous integration testing and CodeCov to check the code coverage of our test suite. The status us TravisCI and CodeCov can be found in README.md, in the top level of our package. Since the test suite is included in the project distribution, users can also install the project package and use pytest and pytest-cov to check the test results locally.\n",
    "\n",
    "### Distribution:\n",
    "Our open-source VorDiff package will be uploaded to PyPI by using twine because it uses a verified connection for secure authentication to PyPI over HTTPS. Users will be able to install our project package by using the convential `pip install VorDiff`.\n",
    "\n",
    "\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### Node\n",
    "The `Node()` class represents a single node in the computational graph of a function. It outlines the operations necessary for the `scalar` and `vector` objects to implement. These two types of variables are separated into their own classes to reduce complexity and improve organization.\n",
    "```py\n",
    "class Node():\n",
    "\n",
    "    def __init__(self, val: float, der: float):\n",
    "        self.val = val\n",
    "        self.der = der\n",
    "\n",
    "    def __add__(self, other):\n",
    "       pass\n",
    "\n",
    "    def __radd__(self, other):\n",
    "       pass\n",
    "\n",
    "    def __mul__(self, other):\n",
    "       pass\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "       pass\n",
    "   ...\n",
    "   ```\n",
    "\n",
    "### Operator\n",
    "The operator class contains all mathematical operations that users can call to build their functions. Each function returns a `Node` object.\n",
    "```py\n",
    "from nodes import node\n",
    "\n",
    "class Operator():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def sqrt(self, x):\n",
    "        pass\n",
    "\n",
    "    def sin(self, x):\n",
    "        pass\n",
    "```\n",
    "\n",
    "The elementary function objects are analytically differentiable and some are defined in the numpy package. We will include the following elementary functions:\n",
    "\n",
    "- Trigonometric functions: sin, cos, tan\n",
    "- Inverse Trigonometric functions: arcsin, arccos, arctan\n",
    "- Hyperbolic functions: sinh, cosh, tanh, arcsinh, arccosh, arctanh\n",
    "- Exponents: exp, exp2, expm1\n",
    "- Logarithms: log, log2, log10, log1p\n",
    "\n",
    "The derivatives of these elementary function objects and the self-defined elementary function objects (such as power and sqrt) can be evaluated with the chain rule.\n",
    "\n",
    "### AutoDiff\n",
    "The `AutoDiff` class will allow the user to easily create variables and build auto-differentiable functions, without having to interface with the `Node` class. It will make use of the auto-differentiator much more intuitive for the user.\n",
    "```py\n",
    "from nodes import Scalar\n",
    "from nodes import Vector\n",
    "\n",
    "class AutoDiff():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def create_scalar_var(self, val: float = 0):\n",
    "        pass\n",
    "\n",
    "    def create_vector_var(self, vals):\n",
    "        pass\n",
    "```\n",
    "\n",
    "### External dependencies:\n",
    "We will use numpy package for mathematical computation (such as sin, log, exp), and pytest and pytest-cov for the test suite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
