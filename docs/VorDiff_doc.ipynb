{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Automatic differentiation is a tool for calculating derivatives using machine accuracy. It has several advantages over traditional methods of derivative calculations such as symbolic and finite differentiation. Automatic differentiation is useful for calculating complex derivatives where errors are more likely with classical methods. For instance , with finite differentiation, h values that are too small will lead to accuracy errors though floating point roundoff error, while h values that are too large will start making vastly inaccurate approximations. \n",
    "\n",
    "Automatic differentiation is useful due to its practicality in real world applications that involve thousands of parameters in a complicated function, which would take a long runtime as well as strong possibility for error in calculating the derivatives individually. \n",
    "\n",
    "Our package allows users to calculate derivatives of complex functions, some with many parameters, allowing machine precision.\n",
    "\n",
    "## Background\n",
    "\n",
    "Essentially automatic differentiation works by  breaking down a complicated function and performing a sequence of elementary arithmetic such as addition, subtraction, multiplication, and division as well as elementary functions like exp, log, sin, etc. These operations are then repeated by the chain rule and the derivatives of these sequences are calculated. There are two ways that automatic differentiation can be implemented - forward mode and reverse mode. \n",
    "\n",
    "\n",
    "### 2.1 The Chain Rule\n",
    "\n",
    "The chain rule makes up a fundamental component of auto differentiation. The basic idea is:   \n",
    "For univariate function, $$ F(x) = f(g(x))$$\n",
    "\n",
    " $$F^{\\prime} = (f(g))^{\\prime} = f^{\\prime}(g(x))g^{\\prime}(x)$$\n",
    " \n",
    "For multivariate function, $$F(x) = f(g(x),h(x))$$\n",
    "\n",
    "$$ \\frac{\\partial F}{\\partial x}=\\frac{\\partial f}{\\partial g} \\frac{\\partial g}{\\partial x}+\\frac{\\partial f}{\\partial h} \\frac{\\partial h}{\\partial x}$$\n",
    "\n",
    "For generalized cases, if F is a combination of more sub-functions,  $$F(x) = f(g_{1}(x), g_{2}(x), …, g_{m}(x))$$\n",
    "\n",
    "$$\\frac{\\partial F}{\\partial x}=\\sum_{i=1}^{m}\\frac{\\partial F}{\\partial g_{i}} \\frac{\\partial g_{i}}{\\partial x}$$\n",
    "\n",
    "For F is a function f(g): f: $R^n$ -> $R^m$ and g: $R^m$ -> $R^k$,\n",
    "\n",
    "$$\\mathbf{J}_{\\mathrm{gof}}(\\mathbf{x})=\\mathbf{J}_{\\mathrm{g}}(\\mathbf{f}(\\mathbf{x})) \\mathbf{J}_{\\mathrm{f}}(\\mathbf{x})$$\n",
    "\n",
    "where $J(f) =\\left[\\begin{array}{ccc}{\\frac{\\partial \\mathbf{f}}{\\partial x_{1}}} & {\\cdots} & {\\frac{\\partial \\mathbf{f}}{\\partial x_{n}}}\\end{array}\\right]=\\left[\\begin{array}{ccc}{\\frac{\\partial f_{1}}{\\partial x_{1}}} & {\\cdots} & {\\frac{\\partial f_{1}}{\\partial x_{n}}} \\\\ {\\vdots} & {\\ddots} & {\\vdots} \\\\ {\\frac{\\partial f_{m}}{\\partial x_{1}}} & {\\cdots} & {\\frac{\\partial f_{m}}{\\partial x_{n}}}\\end{array}\\right]$ is the Jacobian Matrix.\n",
    "\n",
    "\n",
    "\n",
    "### 2.2 Auto Differentiation: Forward Mode\n",
    "\n",
    "The forward mode automatic differentiation is accomplished by firstly splitting the function process into one-by-one steps, each including only one basic operation. It focuses on calculating two things in each step, the value of scalar or vector x in $R^n$, and the 'seed' vector for the derivatives or Jacobian Matrix. From the first node, the value and derivatives will be calculated based on the values and derivatives of forward nodes. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program.\n",
    "\n",
    "The automatic differentiation is superior to analytic or symbolic differentiation because it could be computed on modern machines. It is also superior to numerical differentiation because numeric method is not precise but AD deals with machine precision problems properly.\n",
    "\n",
    "An example of computational graph and table for forward mode AD is shown as follows:\n",
    "\n",
    "\\begin{align}\n",
    "  f\\left(x,y\\right) =\\sin\\left(xy\\right)\n",
    "\\end{align}\n",
    "We will be evaluating the function at $f(1, 0)$\n",
    "\n",
    "Evaluation trace:\n",
    "\n",
    "| Trace   | Elementary Function      | Current Value           | Elementary Function Derivative       | $\\nabla_{x}$ Value  | $\\nabla_{y}$ Value  |\n",
    "| :---: | :-----------------: | :-----------: | :----------------------------: | :-----------------:  | :-----------------: |\n",
    "| $x_{1}$ | $x_{1}$                  | $1$        | $\\dot{x}_{1}$                        | $1$ | $0$ |\n",
    "| $x_{2}$ | $x_{2}$                  | $0$        | $\\dot{x}_{2}$                        | $0$ | $1$ |\n",
    "| $x_{3}$ | $x_{1}x_{2}$                  | $0$        | $\\dot{x}_{2}$                        | $0$ | $1$ |\n",
    "\n",
    "![comp-graph](computationalgraph.png)\n",
    "\n",
    "\n",
    "\n",
    "### 2.3 Reverse Mode\n",
    "\n",
    "The reverse mode automatic differentiation has a process similar to the forward mode auto differentiation, but has another reverse process. It does not apply the chain rule and only partial derivatives to a node are stored. First, for the forward process, the partial derivatives are stored for each node. For the reverse process, it starts with the differentiation to the last node, and then activations in the forward process are deployed in the differentiation step by step. \n",
    "\n",
    "\n",
    "### 2.4 Forward Mode v.s. Reverse Mode\n",
    "\n",
    "Two main aspects can be considered when choosing between Forward and Reverse mode auto differentiation.\n",
    "* Memory Storage & Time of Computation\n",
    "\n",
    "The forward mode needs memory storage for values and derivatives for each node, while the reverse mode only needs to store the activations of partial differentiation to each node. The forward mode do the computation at the same time as the variable evaluation, while the reverse mode do the calculation in the backward process.\n",
    "* Input & Output Dimensionality\n",
    "\n",
    "If the input dimension is much larger than output dimension, then reverse mode is more attractive. If the output dimension is much larger than the input dimension, the forward mode is much computational cheaper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Our package can be installed from our GitHub Repository at: https://github.com/VoraciousFour/cs207-FinalProject.\n",
    "\n",
    "After the package is installed, it needs to be imported into their workspace. Doing so, will automatically download any dependencies that are required by our package such as math or numpy. Then, the user can create and activate a virtual envitronment to use the package in.\n",
    "\n",
    "The user can set up and use our package using their terminal as follows.\n",
    "\n",
    "1. Clone the VorDiff package from our Github Repository into your directory\n",
    "        git clone https://github.com/VoraciousFour/cs207-FinalProject.git\n",
    "2. Create and activate a virtual environment\n",
    "        '''Installing virtualenv'''\n",
    "        sudo easy_install virtualenv\n",
    "        '''Creating the Virtual Environment'''\n",
    "        virtualenv env\n",
    "        '''Activating the Virtual Environment'''\n",
    "        source env/bin/activate\n",
    "3. Install required dependencies\n",
    "        pip install -r requirements.txt\n",
    "4. Importing VorDiff package for use\n",
    "        import VorDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use VorDiff\n",
    "\n",
    "Our Automatic Differentiation package is called VorDiff. The two main objects you will interact with are `AutoDiff` and `Operator`. In short, the user will first instantiate a scalar variable as an `AutoDiff` object, and then feed those variables to operators specified in the `Operator` object. The `Operator` object allows users to build their own functions for auto-differentiation. Simple operations (e.g. addition, multiplication, power) may be used normally. More complex functions (e.g. log, sin, cos) must use the operations defined in the `Operator` class. Lastly, the user may retrieve the values and first derivatives from the objects defined above by using the `get()` method.\n",
    "\n",
    "A short example is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.0000026535897932, -0.9999999999964793)\n",
      "(1.0, 2.0)\n"
     ]
    }
   ],
   "source": [
    "from VorDiff.autodiff import AutoDiff as ad\n",
    "from VorDiff.operator import Operator as op\n",
    "\n",
    "# Define variables\n",
    "x = ad.scalar(3.14159)\n",
    "y = ad.scalar(0)\n",
    "\n",
    "# Build functions\n",
    "fx = op.sin(x) + 3\n",
    "fy = op.exp(y) + op.log(y+1)\n",
    "\n",
    "# Get values and derivates\n",
    "print(fx.get())\n",
    "print(fy.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Organization\n",
    "\n",
    "### Directory Structure\n",
    "The package's directory will be structured as follows:\n",
    "```\n",
    "VorDiff/\n",
    "\t__init__.py\n",
    "    nodes/\n",
    "        __init__.py\n",
    "\t    scalar.py\n",
    "        reverse_scalar.py\n",
    "        reverse_vector.py\n",
    "\t    vector.py\n",
    "\ttests/\n",
    "        __init__.py\n",
    "        test_autodiff.py\n",
    "        test_node.py\n",
    "        test_operator.py\n",
    "        test_reverse_autodiff.py\n",
    "        test_reverse_operator.py\n",
    "        test_reverse_scalar.py\n",
    "        test_reverse_vector.py\n",
    "        test_scaler.py\n",
    "        test_vector.py\n",
    "    autodiff.py\n",
    "    operator.py\n",
    "    reverse_autodiff.py\n",
    "    reverse_operator.py\n",
    "    README.md\n",
    "    ...\n",
    "demo/\n",
    "    demo_reverse.py\n",
    "    demo_scalar.py\n",
    "    demo_vector.py\n",
    "docs/\n",
    "    ...\n",
    "```\n",
    "### Modules\n",
    "-   VorDiff: The VorDiff module contains the operator class to be directly used by users to evaluate functions and calculate their derivatives, and an autodiff class that acts as the central interface for automatic differentiation.\n",
    "\n",
    "-   Nodes: The Nodes module contains the the scalar and vector classes, which define the basic operations that can be performed on scalar and vector variables for the autodiff class.\n",
    "    \n",
    "-   Test_Vordiff: The Test_Vordiff module contains the test suite for this project. TravisCI and CodeCov are used to test our operator classes, node classes, and auto-differentiator.\n",
    "    \n",
    "-   Demo: The Demo module contains python files demonstrating how to perform automatic differentiation with the implemented functions.\n",
    "    \n",
    "### Testing\n",
    "In this project we use TravisCI to perform continuous integration testing and CodeCov to check the code coverage of our test suite. The status us TravisCI and CodeCov can be found in README.md, in the top level of our package. Since the test suite is included in the project distribution, users can also install the project package and use pytest and pytest-cov to check the test results locally.\n",
    "\n",
    "### Distribution:\n",
    "Our open-source VorDiff package will be uploaded to PyPI by using twine because it uses a verified connection for secure authentication to PyPI over HTTPS. Users will be able to install our project package by using the convential `pip install VorDiff`.\n",
    "\n",
    "\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### Scalar\n",
    "The `Scalar` class represents a single scalar node in the computational graph of a function. It implements the interface for user defined scalar variables. The object contains two hidden attributes, `._val` and `._der`, which can be retrieved with the `get()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Docstrings hidden'''\n",
    "class Scalar():\n",
    "\n",
    "    def __init__(self, value, *kwargs):\n",
    "        self._val = value\n",
    "        if len(kwargs) == 0:\n",
    "            self._der = 1\n",
    "        else:\n",
    "            self._der = kwargs[0]\n",
    "    \n",
    "    def get(self):\n",
    "        return self._val, self._der\n",
    "\n",
    "    def __add__(self, other):\n",
    "        try:\n",
    "            return Scalar(self._val+other._val, self._der+other._der)\n",
    "        except AttributeError:\n",
    "            return self.__radd__(other)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return Scalar(self._val+other, self._der)\n",
    "        \n",
    "    '''etc'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector\n",
    "The `Vector` class represents a single vector variable. Vectors are comprised of `Element` objects, which implement much of the computation necessary for vector automatic differentiation. Vectors contain two hidden attributes: a list `_elements`, and a numpy array `_jacob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''Docstrings hidden'''\n",
    "class Vector():\n",
    "\n",
    "    def __init__(self, vec, *kwargs):\n",
    "        self._vec = np.array(vec)\n",
    "        if len(kwargs) == 0:\n",
    "            self._jacob = np.eye(len(vec))\n",
    "        else:\n",
    "            self._jacob = np.array(kwargs[0])\n",
    "            \n",
    "        elements = []\n",
    "        for i in range(len(vec)):\n",
    "            elements.append(Element(self._vec[i], self._jacob[i]))\n",
    "        self._elements = elements\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self._elements[idx]\n",
    "    \n",
    "class Element():\n",
    "    \n",
    "    \"\"\"\n",
    "    The Element object has an evaluated value (it can be the value of function compositions with\n",
    "    the input of user defined values) and a list of current derivatives with respect to each variable.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, val, jacob):\n",
    "        self._val = val\n",
    "        self._jacob = np.array(jacob)\n",
    "\n",
    "    def get_val(self):\n",
    "        return self._val\n",
    "    \n",
    "    def get_derivatives(self):\n",
    "        return self._jacob\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        try:\n",
    "            val = self._val+other._val\n",
    "            jacob = self._jacob+other._jacob\n",
    "            return Element(val, jacob)\n",
    "        \n",
    "        except AttributeError:\n",
    "            return self.__radd__(other)\n",
    "    '''etc'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operator\n",
    "The operator class contains all mathematical operations that users can call to build their functions. Each function returns a `Scalar` object, a `Vector` object, or a numeric constant, depending on the input type. Each function raises an error if its input falls outside its domain. All functions in the class are static."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VorDiff.nodes.scalar import Scalar\n",
    "from VorDiff.nodes.vector import Vector,Element\n",
    "\n",
    "'''Docstrings hidden'''\n",
    "class Operator():\n",
    "    \n",
    "    @staticmethod\n",
    "    def sin(x):\n",
    "        \n",
    "        try:\n",
    "            return Element(np.sin(x._val), np.cos(x._val)*x._jacob)\n",
    "        except AttributeError: # If constant\n",
    "            try: # If scalar variable\n",
    "                return Scalar(np.sin(x._val), x._der*np.cos(x._val))\n",
    "            \n",
    "            except AttributeError: # If constant\n",
    "                return np.sin(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def cos(x):\n",
    "        \n",
    "        try:\n",
    "            return Element(np.cos(x._val), -np.sin(x._val)*x._jacob)\n",
    "        except AttributeError: # If constant\n",
    "            try: # If scalar variable\n",
    "                return Scalar(np.cos(x._val), -np.sin(x._val)*x._der)\n",
    "            \n",
    "            except AttributeError: # If constant\n",
    "                return np.cos(x)\n",
    "    '''etc'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoDiff\n",
    "The `AutoDiff` class will allow the user to easily create variables and build auto-differentiable functions, without having to interface with the any of the node classes. It will make use of the auto-differentiator much more intuitive for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VorDiff.nodes.scalar import Scalar\n",
    "from VorDiff.nodes.vector import Vector, Element\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AutoDiff():\n",
    "    '''\n",
    "    The AutoDiff class allows users to define Scalar variables and \n",
    "    interface with the auto-differentiator.\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def scalar(val):\n",
    "        return Scalar(val, 1)\n",
    "\n",
    "    def element(val,jacob):\n",
    "        return Element(val,jacob)\n",
    "\n",
    "    @staticmethod\n",
    "    def vector(vec):\n",
    "        return Vector(vec, np.eye(len(vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Features (Reverse AD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReverseScalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ReverseScalar` class allows a single user defined variable capable of reverse mode of automatic differentiation. The `ReverseScalar` objects contain two hidden attributes, `._val` and `._gradient`. The attribute `._val` and the function compute_gradient can be retrieved with the `get()` method. The dunder methods are also implemented in this class so that user can do the basic reverse mode computation and operations with the objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseScalar():\n",
    "    \n",
    "    def __init__(self, val: float):\n",
    "        \n",
    "        self._val = val\n",
    "        self._gradient = 1\n",
    "        self._children = {}\n",
    "        \n",
    "    def get(self):\n",
    "        return self._val, self.compute_gradient()\n",
    "    \n",
    "    def compute_gradient(self):\n",
    "        pass\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        pass\n",
    "    'etc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReverseVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ReverseVector` class allows a vector of multiple user defined variables capable of reverse mode of automatic differentiation. The `ReverseVector` objects contain two hidden attributes, `._val` and `._gradient`. The attribute `._val` and can be retrieved with the `get()` method. The partial derivatives with respect to the user defined variables can be retrieved by just calling the attribute `._gradient` of the objects. The dunder methods are also implemented in this class so that user can do the basic reverse mode computation and operations with the objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseVector():\n",
    "   \n",
    "    def __init__(self, vals: list):\n",
    "        self._val = np.array(vals)\n",
    "        self._children = {}\n",
    "        self._gradient = np.zeros(len(vals))\n",
    "        self._reverse_scalars = [ReverseScalar(val) for val in vals]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self._reverse_scalars[idx]\n",
    "    \n",
    "    def get(self):\n",
    "        return self._val\n",
    "    \n",
    "    def _init_children(self):\n",
    "        pass\n",
    "        \n",
    "    def compute_gradient(self, var):\n",
    "        pass\n",
    "            \n",
    "    def __add__(self, other):\n",
    "        pass\n",
    "    '''etc'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Features\n",
    "\n",
    "### 1. Option for higher-order derivatives\n",
    "\n",
    "There are plenty of ways we could improve our package. The first is to grant users the option to compute higher-order derivatives like Hessians. We can recursively apply AD first to the target function, i.e., producing the first-order derivative, then moving the operations of the first-order derivatives into a new computational graph then applying AD again. In short, higher order derivatives would be accomplished by repeatedly applying automatic differentiation to function and its derivatives.\n",
    "\n",
    "### 2. Application using AD library to find the roots of functions\n",
    "\n",
    "A second way we could extend our work is by writing a separate library to find the roots of given functions. For example, this could include an implementation of Newton’s Method that calculates the exact Hessian matrix of a function using AD to get second-order partial derivatives. We would use Newton's Method to search for the approximations by calculating the exact Hessian matrix of the function using AD to get the second-order partial derivatives.\n",
    "\n",
    "### 3. Backpropagation in neural networks\n",
    "\n",
    "We can also extend our implementation of automatic differentiation to the neural networks. Neural networks are able to gradually increase accuracy with every training session through the process of gradient descent. In gradient descent, we aim to minimize the loss (i.e. how inaccurate the model is) through tweaking the weights and biases.\n",
    "\n",
    "By finding the partial derivative of the loss function, we know how much (and in what direction) we must adjust our weights and biases to decrease loss. In that series, we calculate the derivative mean squared error loss function of a single-neuron neural network.\n",
    "\n",
    "For computers to calculate the partial derivatives of an expression in neural networks, we can implement the automatic differentiation for both forward pass and back propagation. Then we can calculate the partial derivatives in both scalar and vector modes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
