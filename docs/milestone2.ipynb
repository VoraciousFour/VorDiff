{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Automatic differentiation is a tool for calculating derivatives using machine accuracy. It has several advantages over traditional methods of derivative calculations such as symbolic and finite differentiation. Automatic differentiation is useful for calculating complex derivatives where errors are more likely with classical methods. For instance , with finite differentiation, h values that are too small will lead to accuracy errors though floating point roundoff error, while h values that are too large will start making vastly inaccurate approximations. \n",
    "\n",
    "Automatic differentiation is useful due to its practicality in real world applications that involve thousands of parameters in a complicated function, which would take a long runtime as well as strong possibility for error in calculating the derivatives individually. \n",
    "\n",
    "Our package allows users to calculate derivatives of complex functions, some with many parameters, allowing machine precision.\n",
    "\n",
    "## Background\n",
    "\n",
    "Essentially automatic differentiation works by  breaking down a complicated function and performing a sequence of elementary arithmetic such as addition, subtraction, multiplication, and division as well as elementary functions like exp, log, sin, etc. These operations are then repeated by the chain rule and the derivatives of these sequences are calculated. There are two ways that automatic differentiation can be implemented - forward mode and reverse mode. \n",
    "\n",
    "\n",
    "### 2.1 The Chain Rule\n",
    "\n",
    "The chain rule makes up a fundamental component of auto differentiation. The basic idea is:   \n",
    "For univariate function, $$ F(x) = f(g(x))$$\n",
    " $$F^{\\prime} = (f(g))^{\\prime} = f^{\\prime}(g(x))g^{\\prime}(x)$$\n",
    "For multivariate function, $$F(x) = f(g(x),h(x))$$\n",
    "$$ \\frac{\\partial F}{\\partial x}=\\frac{\\partial f}{\\partial g} \\frac{\\partial g}{\\partial x}+\\frac{\\partial f}{\\partial h} \\frac{\\partial h}{\\partial x}$$\n",
    "For more generalized cases, if F is a combination of more sub-functions,  $$F(x) = f(g_{1}(x), g_{2}(x), â€¦, g_{m}(x))$$\n",
    "$$\\frac{\\partial F}{\\partial x}=\\sum_{i=1}^{m}\\frac{\\partial F}{\\partial g_{i}} \\frac{\\partial g_{i}}{\\partial x}$$\n",
    "\n",
    "### 2.2 Forward Mode\n",
    "\n",
    "The forward mode automatic differentiation is accomplished by firstly splitting the function process into one-by-one steps, each including only one basic operation. Then from the first node, the value and derivative will be calculated based on the values and derivatives of forward nodes. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program.\n",
    "\n",
    "An example of computational graph and table for forward mode AD is shown as follows:\n",
    "\n",
    "\\begin{align}\n",
    "  f\\left(x,y\\right) =\\sin\\left(xy\\right)\n",
    "\\end{align}\n",
    "We will be evaluating the function at $f(1, 0)$\n",
    "\n",
    "Evaluation trace:\n",
    "\n",
    "| Trace   | Elementary Function      | Current Value           | Elementary Function Derivative       | $\\nabla_{x}$ Value  | $\\nabla_{y}$ Value  |\n",
    "| :---: | :-----------------: | :-----------: | :----------------------------: | :-----------------:  | :-----------------: |\n",
    "| $x_{1}$ | $x_{1}$                  | $1$        | $\\dot{x}_{1}$                        | $1$ | $0$ |\n",
    "| $x_{2}$ | $x_{2}$                  | $0$        | $\\dot{x}_{2}$                        | $0$ | $1$ |\n",
    "| $x_{3}$ | $x_{1}x_{2}$                  | $0$        | $\\dot{x}_{2}$                        | $0$ | $1$ |\n",
    "\n",
    "![comp-graph](computationalgraph.png)\n",
    "\n",
    "\n",
    "\n",
    "### 2.3 Reverse Mode\n",
    "\n",
    "The reverse mode automatic differentiation has a process similar to the forward mode auto differentiation, but has another reverse process. It does not apply the chain rule and only partial derivatives to a node are stored. First, for the forward process, the partial derivatives are stored for each node. For the reverse process, it starts with the differentiation to the last node, and then activations in the forward process are deployed in the differentiation differentiation step by step. \n",
    "\n",
    "\n",
    "### 2.4 Forward Mode v.s. Reverse Mode\n",
    "\n",
    "Two main aspects can be considered when choosing between Forward and Reverse mode auto differentiation.\n",
    "* Memory Storage & Time of Computation\n",
    "\n",
    "The forward mode needs memory storage for values and derivatives for each node, while the reverse mode only needs to store the activations of partial differentiation to each node. The forward mode do the computation at the same time as the variable evaluation, while the reverse mode do the calculation in the backward process.\n",
    "* Input & Output Dimensionality\n",
    "\n",
    "If the input dimension is much larger than output dimension, then reverse mode is more attractive. If the output dimension is much larger than the input dimension, the forward mode is much computational cheaper.\n",
    "\n",
    "\n",
    "## How to use VorDiff\n",
    "\n",
    "Our Automatic Differentiation package is called VorDiff. The two main objects you will interact with are `AutoDiff` and `Operator`. In short, the user will first instantiate a scalar variable as an `AutoDiff` object, and then feed those variables to operators specified in the `Operator` object. The `Operator` object allows users to build their own functions for auto-differentiation. Simple operations (e.g. addition, multiplication, power) may be used normally. More complex functions (e.g. log, sin, cos) must use the operations defined in the `Operator` class. Lastly, the user may retrieve the values and first derivatives from the objects defined above by using the `get()` method.\n",
    "\n",
    "A short example is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.0000026535897932, -0.9999999999964793)\n",
      "(1.0, 2.0)\n"
     ]
    }
   ],
   "source": [
    "from VorDiff.autodiff import AutoDiff as ad\n",
    "from VorDiff.operator import Operator as op\n",
    "\n",
    "# Define variables\n",
    "x = ad.scalar(3.14159)\n",
    "y = ad.scalar(0)\n",
    "\n",
    "# Build functions\n",
    "fx = op.sin(x) + 3\n",
    "fy = op.exp(y) + op.log(y+1)\n",
    "\n",
    "# Get values and derivates\n",
    "print(fx.get())\n",
    "print(fy.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Organization\n",
    "\n",
    "### Directory Structure\n",
    "The package's directory will be structured as follows:\n",
    "```\n",
    "VorDiff\n",
    "\t__init__.py\n",
    "    nodes\n",
    "\t    node.py\n",
    "\t    scalar.py\n",
    "\t    vector.py\n",
    "\ttests\n",
    "\t    test_nodes\n",
    "\t\t    test_node.py\n",
    "\t\t    test_scaler.py\n",
    "\t\t    test_vector.py\n",
    "\t\ttest_autodiff.py\n",
    "        test_operator.py\n",
    "\tdocs\n",
    "\t\t...\n",
    "\tdemo\n",
    "\t    ...\n",
    "\tautodiff.py\n",
    "    operator.py\n",
    "    README.md\n",
    "    ...\n",
    "```\n",
    "### Modules\n",
    "-   VorDiff: The VorDiff module contains the operator class to be directly used by users to evaluate functions and calculate their derivatives, a class node and subclasses scalar and vector to be used in autodiff class, and an autodiff class to perform automatic differentiation. This is the core of the package.\n",
    "    \n",
    "-   Test_Vordiff: The Test_Vordiff module contains the test suite for this project. TravisCI and CodeCov are used to test our operator classes, node classes, and auto-differentiator.\n",
    "    \n",
    "-   Demo: The Demo module contains python files demonstrating how to perform automatic differentiation with the implemented functions.\n",
    "    \n",
    "### Testing\n",
    "In this project we will use TravisCI to perform continuous integration testing and CodeCov to check the code coverage of our test suite. The status us TravisCI and CodeCov can be found in README.md, in the top level of our package. Since the test suite is included in the project distribution, users can also install the project package and use pytest and pytest-cov to check the test results locally.\n",
    "\n",
    "### Distribution:\n",
    "Our open-source VorDiff package will be uploaded to PyPI by using twine because it uses a verified connection for secure authentication to PyPI over HTTPS. Users will be able to install our project package by using the convential `pip install VorDiff`.\n",
    "\n",
    "\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### Scalar\n",
    "The `Scalar` class represents a single scalar node in the computational graph of a function. It implements the interface for user defined scalar variables. The object contains two hidden attributes, `._val` and `._der`, which can be retrieved with the `get()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Documentation Hidden\n",
    "class Scalar():\n",
    "\n",
    "    def __init__(self, value, *kwargs):\n",
    "        self._val = value\n",
    "        if len(kwargs) == 0:\n",
    "            self._der = 1\n",
    "        else:\n",
    "            self._der = kwargs[0]\n",
    "    \n",
    "    def get(self):\n",
    "        return self._val, self._der\n",
    "\n",
    "    def __add__(self, other):\n",
    "        try:\n",
    "            return Scalar(self._val+other._val, self._der+other._der)\n",
    "        except AttributeError:\n",
    "            return self.__radd__(other)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return Scalar(self._val+other, self._der)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        try:\n",
    "            return Scalar(self._val*other._val, self._der*other._val+self._val*other._der)\n",
    "        except AttributeError:\n",
    "            return self.__rmul__(other)\n",
    "        \n",
    "    def __rmul__(self, other):\n",
    "        return Scalar(self._val*other, self._der*other)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        return -self + other\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        try:\n",
    "            return Scalar(self._val/other._val, (self._der*other._val-self._val*other._der)/(other._val**2))\n",
    "        except AttributeError:\n",
    "            return Scalar(self._val/other, self._der/other)\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        return Scalar(other/self._val, other*(-self._der)/(self._val)**2)\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        try:\n",
    "            return Scalar(self._val**other._val, (other._val*self._der/self._val+np.log(self._val)*other._der)*(self._val**other._val))\n",
    "        except AttributeError:\n",
    "            return Scalar(self._val**other, other*(self._val**(other-1))*self._der)\n",
    "            \n",
    "    def __rpow__(self, other):\n",
    "        return Scalar(other**self._val, (other**self._val)*np.log(other)*self._der)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return Scalar((-1)*self._val, (-1)*self._der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operator\n",
    "The operator class contains all mathematical operations that users can call to build their functions. Each function returns a `Scalar` object or a numeric constant, depending on the input type. Each function raises an erro if its input falls outside its domain. All functions in the class are static.\n",
    "\n",
    "In this implementation, we include the following elementary functions. Derivatives are calculated with the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from VorDiff.nodes.scalar import Scalar\n",
    "\n",
    "# Documentation Hidden\n",
    "class Operator():\n",
    "    \n",
    "    @staticmethod\n",
    "    def sin(x):\n",
    "        try: # If scalar variable\n",
    "            return Scalar(np.sin(x._val), x._der*np.cos(x._val))\n",
    "            \n",
    "        except AttributeError: # If contant\n",
    "            return np.sin(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def cos(x):\n",
    "        try: # If scalar variable\n",
    "            return Scalar(np.cos(x._val), -np.sin(x._val)*x._der)\n",
    "            \n",
    "        except AttributeError: # If contant\n",
    "            return np.cos(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def tan(x):\n",
    "        try: # If scalar variable\n",
    "            return Scalar(np.tan(x._val), x._der/np.cos(x._val)**2)\n",
    "            \n",
    "        except AttributeError: # If contant\n",
    "            return np.tan(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def arcsin(x):\n",
    "        try: # If scalar variable\n",
    "            if x._val<-1 or x._val>1:\n",
    "                raise ValueError('out of domain')\n",
    "            else:\n",
    "                return Scalar(np.arcsin(x._val), 1/(x._der*(1-x._val**2)**.5))\n",
    "            \n",
    "        except AttributeError: # If contant\n",
    "            if x<-1 or x>1:\n",
    "                raise ValueError('out of domain')\n",
    "            else:\n",
    "                return np.arcsin(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def arccos(x):\n",
    "        try: # If scalar variable\n",
    "            if x._val<-1 or x._val>1:\n",
    "                raise ValueError('out of domain')\n",
    "            else:\n",
    "                return Scalar(np.arccos(x._val), -x._der/(1-x._val**2)**.5)\n",
    "            \n",
    "        except AttributeError: # If contant\n",
    "            if x<-1 or x>1:\n",
    "                raise ValueError('out of domain')\n",
    "            else:\n",
    "                return np.arccos(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def arctan(x):\n",
    "        try: # If scalar variable\n",
    "            return Scalar(np.arctan(x._val), x._der/(1+x._val**2))\n",
    "            \n",
    "        except: # If contant\n",
    "            return np.arctan(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def log(x):\n",
    "        try: # If scalar variable\n",
    "            return Scalar(np.log(x._val), x._der/x._val)\n",
    "            \n",
    "        except AttributeError: # If contant\n",
    "            return np.log(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def exp(x):\n",
    "        try: # If scalar variable\n",
    "            return Scalar(np.exp(x._val), x._der*np.exp(x._val))\n",
    "            \n",
    "        except AttributeError: # If contant\n",
    "            return np.exp(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def sinh(x):\n",
    "        try: # if scalar variable\n",
    "            return Scalar(np.sinh(x._val), x._der*(np.cosh(x._val)))\n",
    "   \n",
    "        except AttributeError: #if constant\n",
    "            return np.sinh(x)  \n",
    "      \n",
    "    @staticmethod\n",
    "    def cosh(x):\n",
    "        try: # if scalar variable\n",
    "            return Scalar(np.cosh(x._val), x._der*(np.sinh(x._val)))\n",
    "   \n",
    "        except AttributeError: #if constant\n",
    "            return np.cosh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        try: # if scalar variable\n",
    "            return Scalar(np.tanh(x._val), x._der*(1-np.tanh(x._val)**2))\n",
    "   \n",
    "        except AttributeError: #if constant\n",
    "            return np.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def arcsinh(x):\n",
    "        try: # if scalar variable\n",
    "            return Scalar(np.arcsinh(x._val), x._der*(-np.arcsinh(x._val)*np.arctanh(x._val)))\n",
    "   \n",
    "        except AttributeError: #if constant\n",
    "            return np.arcsinh(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def arccosh(x):\n",
    "        try: # if scalar variable\n",
    "            return Scalar(np.arccosh(x._val), x._der*(-np.arccosh(x._val)*np.tanh(x._val)))\n",
    "   \n",
    "        except AttributeError: #if constant\n",
    "            return np.arccosh(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def arctanh(x):\n",
    "        try: # if scalar variable\n",
    "            return Scalar(np.arctanh(x._val), x._der*(1-np.arctanh(x._val)**2))\n",
    "   \n",
    "        except AttributeError: #if constant\n",
    "            return np.arctanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoDiff\n",
    "The `AutoDiff` class will allow the user to easily create variables and build auto-differentiable functions, without having to interface with the `Node` class. It will make use of the auto-differentiator much more intuitive for the user. It will become much more important when we implement `Vector` nodes, in addition to just `Scalar` nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VorDiff.nodes.scalar import Scalar\n",
    "\n",
    "# Documentation Hidden\n",
    "class AutoDiff():\n",
    "\n",
    "    @staticmethod\n",
    "    def scalar(val):\n",
    "        \n",
    "        return Scalar(val, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Our package can be installed from our GitHub Repository at: https://github.com/VoraciousFour/cs207-FinalProject.\n",
    "\n",
    "After the package is installed, it needs to be imported into their workspace. Doing so, will automatically download any dependencies that are required by our package such as math or numpy. Then, the user can create and activate a virtual envitronment to use the package in.\n",
    "\n",
    "The user can set up and use our package using their terminal as follows.\n",
    "\n",
    "1. Clone the VorDiff package from our Github Repository into your directory\n",
    "        git clone https://github.com/VoraciousFour/cs207-FinalProject.git\n",
    "2. Create and activate a virtual environment\n",
    "        '''Installing virtualenv'''\n",
    "        sudo easy_install virtualenv\n",
    "        '''Creating the Virtual Environment'''\n",
    "        virtualenv env\n",
    "        '''Activating the Virtual Environment'''\n",
    "        source env/bin/activate\n",
    "3. Install required dependencies\n",
    "        pip install -r requirements.txt5\n",
    "4. Importing VorDiff package for use\n",
    "        import VorDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Features\n",
    "\n",
    "1. Method to install VorDiff using Python Package Index (PyPl) \n",
    "\n",
    "\n",
    "2. Jacobian will be changed to support multiple functions of multiple inputs instead of just scalar function of single input\n",
    "\n",
    " Adding a method to install VorDiff with PyPl will be simple as we will       navigate the pypi website and publish VorDiff there.\n",
    "\n",
    " Changing the Jacobian to support multiple functions will require us to       change some of our code in our implementations to work with multiple         functions and inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
